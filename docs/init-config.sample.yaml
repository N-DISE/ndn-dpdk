# NDN-DPDK initialization configuration
---

# MEMORY POOL
# 'Capacity' affects hugepage memory usage; (2^q-1) is optimal.
# Each NUMA socket has its own mempool with the given name.
Mempool:
  DIRECT:  # RX incoming packets
    Capacity: 524287
    Dataroom: 2176
  INDIRECT:  # indirect mbufs
    Capacity: 1048575
  NAME:  # name linearize
    Capacity: 65535
  HEADER:  # TX packet headers
    Capacity: 65535
  GUIDER:  # TX modified Interest guiders (forwarder only)
    Capacity: 65535
  INT:  # TX Interests (trafficgen only)
    Capacity: 65535
  DATA:  # TX Data hader (trafficgen only)
    Capacity: 65535
  PAYLOAD:  # TX Data payload (trafficgen only)
    Capacity: 255

# LCORE ALLOCATION
# This is a mapping from LCore role to its reservations.
# Roles for ndnfw-dpdk: RX, TX, CRYPTO, FWD.
# Roles for ndnping-dpdk: RX, TX, SVR, CLIR, CLIT.
# Within each role:
# 'lcores' is a list of lcores reserved for a role.
# 'pernuma' is a map of { NumaSocket => max number of lcores } for a role.
# Leaving this section blank results in default allocation.
LCoreAlloc:
# ROLE1: # this role has lcore 1 and lcore 3
#   lcores: [1, 3]
# ROLE2: # this role can have up to 3 and 2 lcores on two NUMA sockets
#   pernuma:
#     0: 3
#     1: 2

# FACE CREATION
Face:
  # whether to enable Ethernet faces
  EnableEth: true
  # whether to disable RxFlow dispatching
  EthDisableRxFlow: false
  # Ethernet device MTU
  EthMtu: 1500
  # Ethernet RX queue capacity
  EthRxqFrames: 4096
  # Ethernet before-TX queue capacity
  EthTxqPkts: 256
  # Ethernet after-TX queue capacity
  EthTxqFrames: 4096

  # whether to enable socket faces
  EnableSock: true
  # socket before-TX queue capacity
  SockTxqPkts: 256
  # socket after-TX queue capacity
  SockTxqFrames: 1024

  # ChanRxGroup queue capacity (shared among all socket/mock faces)
  ChanRxgFrames: 4096

# NDT
Ndt:
  # Names are dispatched using prefix with this number of components.
  PrefixLen: 2
  # There are (2^IndexBits) elements.
  IndexBits: 16
  # Collect per-element sample every (2^SampleFreq) lookups. Must not exceed 30.
  SampleFreq: 8

# FIB
Fib:
  # Capacity in each partition. Affects hugepage memory usage.
  # (2^q-1) is optimal.
  MaxEntries: 65535
  # Number of hash table buckets in each partition. Must be power of 2.
  NBuckets: 256
  # 'M' parameter in 2-stage LPM algorithm.
  # Should be greater than length of most FIB entry names.
  StartDepth: 8

# FORWARDER DATA PLANE
Fwdp:
  # Settings of Interest queue between FwInput and FwFwd.
  FwdInterestQueue:
    Capacity: 131072
    DequeueBurstSize: 32
  # Settings of Data queue between FwInput and FwFwd.
  FwdDataQueue:
    Capacity: 131072
    DequeueBurstSize: 64
  # Settings of Nack queue between FwInput and FwFwd.
  FwdNackQueue:
    Capacity: 131072
    DequeueBurstSize: 64
  # Collect RX-FwFwd latency sample every 2^LatencySampleFreq packets.
  # Must not exceed 30.
  LatencySampleFreq: 16
  # PIT suppression config.
  Suppress:
    Min: 10ms
    Max: 100ms
    Multiplier: 2.0
  # Capacity of PCCT. (2^q-1) is optimal.
  PcctCapacity: 131071
  # Capacity of CS in-memory direct entries. Twice as many PCCT entries could
  # be occupied due to ghost lists in ARC algorithm.
  CsCapMd: 32768
  # Capacity of CS in-memory indirect entries.
  CsCapMi: 32768
